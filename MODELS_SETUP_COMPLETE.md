# âœ… MODELS READY FOR DOWNLOAD & INTEGRATION

Your NovaAI project is now **fully integrated** with Mistral and Stable Diffusion. Here's what has been set up:

## ğŸ“¦ What's Installed

âœ… **Model Manager** (`backend/model_manager.py`)
- Centralized model initialization
- Lazy loading for Stable Diffusion
- Ollama service monitoring
- Automatic optimization for CPU/GPU

âœ… **Updated Image Router** (`backend/routers/image.py`)
- Integrated with Model Manager
- Error handling for low memory
- Fallback mechanisms

âœ… **Main App Updates** (`backend/main.py`)
- Models initialize on startup
- Proper logging configured

âœ… **Setup Scripts**
- `setup_models.py` - Full setup wizard
- `setup_models.bat` - Windows batch helper
- `QUICK_START.md` - Comprehensive guide
- `SETUP_MODELS.md` - Detailed instructions

---

## ğŸš€ QUICK START (Choose One Method)

### Method 1ï¸âƒ£: **Automatic Setup** (Recommended)

```powershell
cd "c:\Users\uha\OneDrive\Desktop\final year project"
.\venv\Scripts\Activate.ps1
python setup_models.py
```

This will:
1. âœ… Download Stable Diffusion v1-5 (~5GB) - ~10-20 min
2. âœ… Configure Ollama with Mistral (manual step prompted)
3. âœ… Verify everything is ready

### Method 2ï¸âƒ£: **Manual Setup** (If auto-download fails)

#### Step A: Install Ollama + Mistral

```powershell
# 1. Download installer from https://ollama.ai
# 2. Run installer and complete setup
# 3. Ollama starts as Windows service automatically
# 4. In PowerShell, pull Mistral:
ollama pull mistral

# Verify it works:
ollama run mistral
# Type: exit
```

#### Step B: Download Stable Diffusion

```powershell
cd "c:\Users\uha\OneDrive\Desktop\final year project"
.\venv\Scripts\Activate.ps1

# Option 1: Using Python (one-time download)
python -c "from diffusers import StableDiffusionPipeline; StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')"

# Option 2: Using Hugging Face CLI (if preferred)
pip install huggingface-hub[cli]
huggingface-cli download runwayml/stable-diffusion-v1-5
```

#### Step C: Start the App

```powershell
python -m uvicorn backend.main:app --host 127.0.0.1 --port 8000 --reload
```

---

## ğŸ“‹ System Requirements

| Requirement | Minimum | Recommended |
|------------|---------|-------------|
| **RAM** | 8GB | 16GB+ |
| **CPU** | Dual-core | 4+ cores |
| **Storage** | 15GB free | 20GB free |
| **GPU** | Any | NVIDIA w/ CUDA |
| **Internet** | Stable | Broadband |

---

## ğŸ” Verify Setup Status

After downloading models:

```powershell
# Check Ollama service
curl http://localhost:11434/api/tags

# Test Ollama with Mistral:
ollama run mistral
# Type message, then: exit

# Check Python access to Stable Diffusion:
python -c "from diffusers import StableDiffusionPipeline; print('âœ… Stable Diffusion ready')"
```

---

## ğŸ¯ How Models Are Integrated

### Chat Feature (Mistral via Ollama)

**Flow:**
```
User Message 
      â†“
FastAPI Endpoint (/api/chat/generate)
      â†“
HTTP Request to Ollama (localhost:11434)
      â†“
Mistral Model Inference
      â†“
JSON Response
      â†“
Save to Database
      â†“
Return to Frontend
```

**Code location:** `backend/routers/chat.py`

### Image Generation (Stable Diffusion)

**Flow:**
```
User Prompt
      â†“
FastAPI Endpoint (/api/image/generate)
      â†“
Model Manager (backend/model_manager.py)
      â†“
Load from HuggingFace Cache
      â†“
Stable Diffusion Inference
      â†“
Save PNG to static/generated_images/
      â†“
Return URL to Frontend
```

**Code location:** `backend/routers/image.py`

---

## ğŸ’¡ Usage Examples

### ğŸ’¬ Chat with Mistral

```bash
# Terminal 1: Start Ollama service
ollama serve

# Terminal 2: Start the app
python -m uvicorn backend.main:app --reload

# Browser: http://127.0.0.1:8000
# â†’ Login
# â†’ Navigate to /chat
# â†’ Select "mistral"
# â†’ Send message
# â†’ Wait 5-30 seconds for response
```

### ğŸ¨ Generate Images

```bash
# Same setup as above, then:
# Browser: http://127.0.0.1:8000
# â†’ Login
# â†’ Navigate to /image-generator
# â†’ Enter prompt: "A serene landscape with mountains, sunset"
# â†’ Click Generate
# â†’ Wait 30-120 seconds (CPU) or 10-30 seconds (GPU)
# â†’ Image appears and saves to static/generated_images/
```

---

## âš™ï¸ Configuration Options

### Modify Image Generation Quality

Edit `backend/routers/image.py` line ~25:

```python
# Lower = faster, lower quality
# Higher = slower, higher quality
image = pipe(request.prompt, num_inference_steps=50).images[0]
```

Recommended:
- **Fast:** 20-30 steps
- **Balanced:** 40-50 steps
- **High Quality:** 60-80 steps

### Use GPU if Available

```powershell
# Install PyTorch with CUDA (for NVIDIA GPUs)
pip uninstall torch -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Verify:
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

### Use Different Ollama Models

```powershell
# List available models
ollama list

# Try another model
ollama pull llama2
ollama pull neural-chat

# In app, select from dropdown in /chat page
```

---

## ğŸ”§ Troubleshooting

### Issue: "Ollama not reachable"
```powershell
# Make sure Ollama is running:
ollama serve  # Run in separate terminal
```

### Issue: "Stable Diffusion download stuck"
```powershell
# Clear cache and retry:
rm -r "$env:USERPROFILE\.cache\huggingface"
python -c "from diffusers import StableDiffusionPipeline; StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')"
```

### Issue: "Out of Memory"
```powershell
# Reduce steps:
# In backend/routers/image.py, change num_inference_steps to 20-30
# Or use GPU with CUDA
```

### Issue: "Download timeout"
```powershell
# Download takes time, leave terminal running
# Or set longer timeout:
set HF_HUB_DOWNLOAD_TIMEOUT=600
python -c "from diffusers import StableDiffusionPipeline; StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')"
```

---

## ğŸ“Š Directory Structure

```
project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ model_manager.py          â† Model management
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ chat.py               â† Ollama integration
â”‚   â”‚   â””â”€â”€ image.py              â† Stable Diffusion integration
â”‚   â”œâ”€â”€ main.py                   â† Auto-initialization
â”‚   â””â”€â”€ ...
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ generated_images/         â† Saved generated images
â”‚   â””â”€â”€ ...
â”œâ”€â”€ setup_models.py               â† Setup wizard
â”œâ”€â”€ setup_models.bat              â† Windows helper
â”œâ”€â”€ QUICK_START.md                â† This guide
â””â”€â”€ SETUP_MODELS.md               â† Detailed setup
```

---

## âœ¨ What's Next

1. **Download models** - Use setup_models.py or follow manual steps
2. **Start services** - Ollama serve + FastAPI app
3. **Login** - Create account at http://127.0.0.1:8000/signup
4. **Test Chat** - Ask Mistral a question
5. **Test Images** - Generate an image with prompt
6. **Explore** - Try video editor and other features

---

## ğŸ“ Learning Resources

- **Ollama**: https://ollama.ai (Model management)
- **Hugging Face**: https://huggingface.co (Model hub)
- **Diffusers**: https://huggingface.co/docs/diffusers (Image generation)
- **FastAPI**: https://fastapi.tiangolo.com (Backend framework)

---

## ğŸ“ Support

If models won't download:
1. Check internet connection
2. Try manual download with `python setup_models.py`
3. Ensure 15GB+ free disk space
4. Check firewall/VPN isn't blocking HuggingFace
5. Try clearing cache: `rm -r "$env:USERPROFILE\.cache\huggingface"`

**Status:** âœ… All integrations complete. Ready for model download!

---

**Last Updated:** February 10, 2026

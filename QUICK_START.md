# ğŸš€ NovaAI - Model Installation & Integration

This document explains how to set up both **Mistral** and **Stable Diffusion** for your NovaAI project.

## What We've Set Up

âœ… **Mistral LLM** - Integrated with Ollama for intelligent chat
âœ… **Stable Diffusion v1-5** - Local image generation  
âœ… **Model Manager** - Central module for model initialization and access
âœ… **Auto-initialization** - Models load automatically on app startup

---

## Quick Start (3 Steps)

### Step 1ï¸âƒ£ Install Ollama

Download and install from: **https://ollama.ai**

This provides Mistral for your chatbot.

### Step 2ï¸âƒ£ Run Model Setup Script

Open PowerShell in your project directory:

```powershell
cd "c:\Users\uha\OneDrive\Desktop\final year project"
.\venv\Scripts\Activate.ps1
python setup_models.py
```

OR use the batch file:
```powershell
.\setup_models.bat
```

This will:
- Download and cache Stable Diffusion v1-5 model (~4GB)
- Configure Mistral in Ollama (~4GB)
- Verify both models are ready

### Step 3ï¸âƒ£ Start the Server

```powershell
python -m uvicorn backend.main:app --host 127.0.0.1 --port 8000 --reload
```

**Visit:** http://127.0.0.1:8000

---

## Architecture Overview

### Model Manager (`backend/model_manager.py`)
Central module for all AI models:
- âœ… Lazy loads Stable Diffusion pipeline
- âœ… Checks Ollama service availability
- âœ… Provides error handling and fallbacks
- âœ… Optimizes for CPU and GPU

### Image Router (`backend/routers/image.py`)
Updated to:
- âœ… Use the model manager for SD pipeline
- âœ… Handle GPU/CPU memory errors gracefully
- âœ… Create fallback placeholder images on error
- âœ… Log detailed error messages

### Chat Router (`backend/routers/chat.py`)
Already configured to:
- âœ… Call Ollama API at `http://localhost:11434`
- âœ… Support multiple models (mistral, llama2, etc.)
- âœ… Store chat history in SQLite
- âœ… Handle streaming responses

### Main App (`backend/main.py`)
Updated to:
- âœ… Initialize models on startup
- âœ… Mount static files and templates correctly
- âœ… Log application lifecycle events

---

## System Requirements

**Minimum:**
- CPU: Dual-core processor
- RAM: 8GB
- Storage: 15GB free (for models)
- Internet: Stable connection (one-time for downloads)

**Recommended:**
- CPU: Modern 4+ core processor
- RAM: 16GB+
- GPU: NVIDIA with CUDA support
- SSD: For faster model loading

**GPU Setup:**
If you have an NVIDIA GPU, install PyTorch with CUDA:
```powershell
pip uninstall torch -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

---

## File Structure

```
final year project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ model_manager.py          â† Central model management
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ chat.py               â† Uses Ollama (Mistral)
â”‚   â”‚   â””â”€â”€ image.py              â† Uses Stable Diffusion
â”‚   â”œâ”€â”€ main.py                   â† Model initialization on startup
â”‚   â””â”€â”€ ...other files
â”œâ”€â”€ setup_models.py               â† Download & cache models
â”œâ”€â”€ setup_models.bat              â† Windows batch setup helper
â”œâ”€â”€ SETUP_MODELS.md               â† Detailed setup guide
â””â”€â”€ QUICK_START.md                â† This file
```

---

## Usage

### ğŸ’¬ Chat Feature (Mistral)

1. Navigate to: **http://127.0.0.1:8000/chat**
2. Login with your account
3. Select model: **mistral** (or llama2 if available)
4. Type your message
5. Wait for response (5-30 seconds)

**Example prompts:**
- "What is machine learning?"
- "Write a Python function to calculate factorial"
- "Explain quantum computing in simple terms"

### ğŸ¨ Image Generation (Stable Diffusion)

1. Navigate to: **http://127.0.0.1:8000/image-generator**
2. Login with your account
3. Enter a detailed prompt
4. Click "Generate"
5. Wait for result (30-120 seconds on CPU, 10-30 on GPU)

**Example prompts:**
- "A beautiful sunset over mountains, oil painting"
- "A steampunk robot in a library, highly detailed"
- "A cozy coffee shop in autumn, digital art"

---

## Monitoring & Debugging

### Check Model Status

```powershell
# Check if Ollama is running
curl http://localhost:11434/api/tags

# List available models in Ollama
ollama list

# Check CUDA availability
python -c "import torch; print(torch.cuda.is_available())"
```

### View Server Logs

The server logs in the console show:
- Model initialization status
- Request processing
- Error details with full tracebacks

### Database Files

- **Database**: `database/` directory (or root if using SQLite)
- **Generated images**: `static/generated_images/`
- **Chat history**: Stored in SQLite database via `ChatHistory` model

---

## Troubleshooting

### âŒ "Ollama not reachable"

**Solution:**
```powershell
# Start Ollama service
ollama serve

# In another terminal, pull fresh model
ollama pull mistral
```

### âŒ "Stable Diffusion failed to load"

**Solution:**
```powershell
# Clear cache and re-download
rm -r "$env:USERPROFILE\.cache\huggingface"
python setup_models.py
```

### âŒ "CUDA out of memory"

**Solution:**
Reduce `num_inference_steps` in `backend/routers/image.py` (default 50 â†’ try 20-30)

### âŒ "Download timeout"

**Solution:**
Network interrupted. Delete cache and try again:
```powershell
rm -r "$env:USERPROFILE\.cache\huggingface"
python setup_models.py
```

---

## Performance Tips

| Setting | Speed | Quality | RAM Used |
|---------|-------|---------|----------|
| 20 steps | âš¡âš¡âš¡ Fast | â­â­ | 2GB |
| 50 steps | âš¡âš¡ Medium | â­â­â­ | 4GB |
| 75 steps | âš¡ Slow | â­â­â­â­ | 6GB |

Modify in `backend/routers/image.py` line 25:
```python
image = pipe(request.prompt, num_inference_steps=50).images[0]  # 50 = default
```

---

## Next Steps

After setup is complete:

1. âœ… Test chat: Ask Mistral a simple question
2. âœ… Test image generation: Create 1-2 images to verify
3. âœ… Create accounts: Test authentication
4. âœ… Check persistence: Verify chat history and images save
5. âœ… Explore: Try the video editor and other features

---

## Additional Resources

- **Ollama**: https://ollama.ai
- **Hugging Face Diffusers**: https://huggingface.co/docs/diffusers
- **Mistral Model**: https://huggingface.co/mistralai
- **Stable Diffusion**: https://huggingface.co/runwayml

---

## Support

If you encounter issues:

1. Check logs in the terminal for detailed error messages
2. Verify both Ollama and FastAPI are running
3. Ensure you have 15GB+ free disk space
4. Check internet connection for model downloads
5. Try restarting both services

---

**Last Updated:** February 10, 2026

ğŸ‰ Your NovaAI instance is now ready with full AI capabilities!
